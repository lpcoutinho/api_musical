{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "TINY_TOKEN = os.getenv(\"TINY_TOKEN\")\n",
    "HOST = os.getenv(\"HOST\")\n",
    "POSTGRES_DB = os.getenv(\"POSTGRES_DB\")\n",
    "POSTGRES_USER = os.getenv(\"POSTGRES_USER\")\n",
    "POSTGRES_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "\n",
    "# Informações de conexão com o banco de dados PostgreSQL\n",
    "db_config = {\n",
    "    \"host\": 'localhost',\n",
    "    \"database\": POSTGRES_DB,\n",
    "    \"user\": POSTGRES_USER,\n",
    "    \"password\": POSTGRES_PASSWORD,\n",
    "}\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Registra o tempo antes da execução\n",
    "start_prog = time.time()\n",
    "\n",
    "# Configurações da API Tiny\n",
    "\n",
    "token = TINY_TOKEN\n",
    "formato = 'JSON'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando IDs de produtos\n",
    "\n",
    "excel_path = r\"..\\Data\\Base\\Envios Full.xlsx\"  # Caminho para o arquivo Excel\n",
    "\n",
    "sheet_name = \"Relação Full x Tiny\"  # Nome da planilha\n",
    "\n",
    "col1 = \"ID Tiny\"  # Nome da coluna a carregar\n",
    "col2 = \"Título do anúncio\"\n",
    "\n",
    "# Use o Pandas para ler a coluna específica\n",
    "df_tiny_id_original = pd.read_excel(excel_path, sheet_name=sheet_name, usecols=[col1, col2])\n",
    "\n",
    "# Salve o DataFrame original\n",
    "df_tiny_id = df_tiny_id_original.copy()\n",
    "\n",
    "# Agora, df conterá apenas a coluna especificada\n",
    "df_tiny_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Converte para números inteiros e substitui os não numéricos por NaN\n",
    "# df_tiny_id['ID Tiny'] = pd.to_numeric(df_tiny_id['ID Tiny'], errors='coerce')\n",
    "\n",
    "# df_tiny_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remova as linhas com valores NaN - Linha 378\n",
    "df_tiny_id_no_nan = df_tiny_id.dropna(subset=['ID Tiny'])\n",
    "\n",
    "df_tiny_id_no_nan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Salve o DataFrame original\n",
    "# df_tiny_id_original = df_tiny_id.copy()\n",
    "\n",
    "# # Remova as linhas com valores NaN\n",
    "# df_tiny_id_no_nan = df_tiny_id.dropna(subset=['ID Tiny'])\n",
    "\n",
    "# # Compare os DataFrames para encontrar a linha removida\n",
    "# linha_removida = df_tiny_id_original[~df_tiny_id_original.index.isin(df_tiny_id_no_nan.index)]\n",
    "\n",
    "# # Exiba a linha removida\n",
    "# print(linha_removida)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica se há duplicatas\n",
    "duplicatas = df_tiny_id_no_nan.duplicated()\n",
    "numero_de_duplicatas = duplicatas.sum()\n",
    "\n",
    "print(f'Número de duplicatas: {numero_de_duplicatas}')\n",
    "\n",
    "# # Encontre as duplicatas no DataFrame\n",
    "# duplicatas = df_tiny_id_no_nan[df_tiny_id_no_nan.duplicated(keep=False)]\n",
    "\n",
    "# # Exiba as próprias duplicatas\n",
    "# duplicatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove as duplicatas e atualiza o DataFrame\n",
    "df_tiny_id_no_dup = df_tiny_id_no_nan.drop_duplicates()\n",
    "\n",
    "df_tiny_id_no_dup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requisitando produtos à API Tiny\n",
    "\n",
    "start_time = time.time() # Registra o tempo antes da execução\n",
    "\n",
    "url = 'https://api.tiny.com.br/api2/produto.obter.php'\n",
    "\n",
    "# df_tiny_id_no_dup = df_tiny_id_no_dup.head(3)\n",
    "#df_tiny_id = pd.read_csv('tiny_id_no_dup.csv')\n",
    "df_tiny_id = df_tiny_id_no_dup\n",
    "\n",
    "responses = []  # Lista para armazenar os resultados\n",
    "num = 0\n",
    "\n",
    "def enviarREST(url, data, optional_headers=None):\n",
    "    headers = optional_headers if optional_headers is not None else {}\n",
    "\n",
    "    response = requests.post(url, data=data, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Problema com {url}, Status Code: {response.status_code}\")\n",
    "\n",
    "    return response.text\n",
    "\n",
    "\n",
    "for id in df_tiny_id['ID Tiny']:\n",
    "    print(f'Buscando dados de {id}')\n",
    "    num +=1\n",
    "    print(f'Loop: {num}')\n",
    "    data = {'token': token, 'id': id, 'formato': formato}\n",
    "    response = enviarREST(url, data)\n",
    "    responses.append(response)\n",
    "\n",
    "    # Verifica se é múltiplo de 50 para aguardar 1min a cada 50 requisições\n",
    "    if num % 50 == 0:\n",
    "        print(\"Esperando 1 minuto...\")\n",
    "        time.sleep(60)  # Pausa por 1 minuto\n",
    "\n",
    "# Registra o tempo depois da execução\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcula o tempo decorrido\n",
    "elapsed_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tempo :decorrido {elapsed_time / 60} minutos\")\n",
    "print(f\"Total de resultados encontrados: {len(responses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gravando respostas em txt\n",
    "datetime_now = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "file_path = f\"../Data/Output/tiny_produtos_json_{datetime_now}.txt\"\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    for line in responses:\n",
    "        file.write(line + '\\n')  # Adicione uma quebra de linha\n",
    "\n",
    "print(f'Os dados foram gravados no arquivo: {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificandos e há erros de processamento\n",
    "text_search1 = '\"status_processamento\":\"1\"'\n",
    "text_search2 = '\"status_processamento\":\"2\"'\n",
    "\n",
    "lines_search1 = []\n",
    "lines_search2 = []\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for n_line, line in enumerate(file, start=1):\n",
    "        if text_search1 in line:\n",
    "            lines_search1.append(n_line)\n",
    "        if text_search2 in line:\n",
    "            lines_search2.append(n_line)\n",
    "\n",
    "if lines_search1 and lines_search2:\n",
    "    print(f'\"status_processamento\":\"1\" foi encontrado nas linhas: {lines_search1}')\n",
    "    print(f'\"status_processamento\":\"2\" foi encontrado nas linhas: {lines_search2}')\n",
    "elif lines_search1:\n",
    "    print(f'\"status_processamento\":\"1\" foi encontrado nas linhas: {lines_search1}')\n",
    "    print('\"status_processamento\":\"2\" não foi encontrado em nenhuma linha.')\n",
    "elif lines_search2:\n",
    "    print(f'\"status_processamento\":\"2\" foi encontrado nas linhas: {lines_search2}')\n",
    "    print('\"status_processamento\":\"1\" não foi encontrado em nenhuma linha.')\n",
    "else:\n",
    "    print('Não foi verificado erro de processamento\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Df Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = responses\n",
    "\n",
    "tiny_prod_df = pd.DataFrame() # Inicialize o DataFrame vazio\n",
    "\n",
    "for json_str in json_list:\n",
    "    json_data = json.loads(json_str) # Transforme a string JSON em um objeto Python\n",
    "\n",
    "    # Extrair a parte \"produto\" do JSON\n",
    "    produto = json_data[\"retorno\"][\"produto\"]\n",
    "    # print(produto)\n",
    "    \n",
    "    # Adicione os dados extraídos ao DataFrame\n",
    "    tiny_prod_df = pd.concat([tiny_prod_df, pd.DataFrame([produto])], ignore_index=True)\n",
    "\n",
    "# Exibir o DataFrame\n",
    "tiny_prod_df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tiny_prod_df.shape)\n",
    "\n",
    "remove_cols = ['anexos', 'imagens_externas']\n",
    "\n",
    "tiny_prod_df = tiny_prod_df.drop(columns=remove_cols)\n",
    "\n",
    "print(tiny_prod_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica se há duplicatas\n",
    "duplicatas = tiny_prod_df.duplicated()\n",
    "total_duplicatas = duplicatas.sum() \n",
    "\n",
    "print(f'Número de duplicatas: {total_duplicatas}')\n",
    "\n",
    "# Encontra duplicatas no DataFrame\n",
    "duplicatas = tiny_prod_df[tiny_prod_df.duplicated(keep=False)]\n",
    "\n",
    "# Exiba duplicatas\n",
    "duplicatas['id']\n",
    "\n",
    "l_duplicatas = duplicatas[duplicatas['id'] == '565663877']\n",
    "l_duplicatas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria DataFrame booleano para verificar se cada célula contém uma lista\n",
    "is_list_df = tiny_prod_df.applymap(lambda x: isinstance(x, list))\n",
    "\n",
    "# Verifica se há alguma coluna que contenha pelo menos uma lista\n",
    "colunas_com_listas = is_list_df.any()\n",
    "\n",
    "# Exibe colunas que contêm listas\n",
    "colunas_com_listas = colunas_com_listas[colunas_com_listas]\n",
    "colunas_com_listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove as duplicatas e atualiza o DataFrame\n",
    "tiny_prod_no_dup = tiny_prod_df.drop_duplicates()\n",
    "\n",
    "print(tiny_prod_df.shape)\n",
    "print(tiny_prod_no_dup.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvando na base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "conn = psycopg2.connect(**db_config)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "n = 1\n",
    "\n",
    "# Itere sobre as linhas do DataFrame e insira os dados na tabela \"tiny_products\"\n",
    "for index, row in tiny_prod_no_dup.iterrows():\n",
    "    print(f'Loop nº: {n}')\n",
    "    query = \"\"\"\n",
    "    INSERT INTO tiny_products (tiny_id,nome,sku_tiny,unidade,preco,ncm,origem,gtin,peso_bruto,estoque_minimo,estoque_maximo,id_fornecedor,nome_fornecedor,\n",
    "    codigo_pelo_fornecedor,preco_custo,preco_custo_medio,situacao,tipo,cest,marca,tipo_embalagem,altura_embalagem,comprimento_embalagem,largura_embalagem,\n",
    "    diametro_embalagem,qtd_volumes,categoria)\n",
    "    VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s);\n",
    "    \"\"\"\n",
    "    values = (\n",
    "        row['id'],\n",
    "        row['nome'],\n",
    "        row['codigo'],\n",
    "        row['unidade'],\n",
    "        row['preco'],\n",
    "        row['ncm'],\n",
    "        row['origem'],\n",
    "        row['gtin'],\n",
    "        row['peso_bruto'],\n",
    "        row['estoque_minimo'],\n",
    "        row['estoque_maximo'],\n",
    "        row['id_fornecedor'],\n",
    "        row['nome_fornecedor'],\n",
    "        row['codigo_pelo_fornecedor'],\n",
    "        row['preco_custo'],\n",
    "        row['preco_custo_medio'],\n",
    "        row['situacao'],\n",
    "        row['tipo'],\n",
    "        row['cest'],\n",
    "        row['marca'],\n",
    "        row['tipoEmbalagem'],\n",
    "        row['alturaEmbalagem'],\n",
    "        row['comprimentoEmbalagem'],\n",
    "        row['larguraEmbalagem'],\n",
    "        row['diametroEmbalagem'],\n",
    "        row['qtd_volumes'],\n",
    "        row['categoria']\n",
    "    )\n",
    "    # print(values)\n",
    "    n += 1\n",
    "    cursor.execute(query, values)\n",
    "\n",
    "# Faça o commit para salvar as alterações no banco de dados\n",
    "conn.commit()\n",
    "\n",
    "# Feche o cursor e a conexão\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print('Produtos adicionados ao Banco de dados.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
